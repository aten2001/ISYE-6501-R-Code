################################################################################

## Problem 3.1 (a)
install.packages("kknn")
install.packages("caret")
library(caret)
library(kknn)
library(ggplot2)

## Import the data
data_cv <- read.table("credit_card_data.txt")

head(data_cv)

set.seed(1)
## Take a random sample of 80% of the data aside for training
mask_test <- sample(nrow(data_cv), size = floor(nrow(data_cv) * 0.2))
test_cv <- data_cv[mask_test,]
train_cv <- data_cv[-mask_test,]

set.seed(1)
## Shuffle training data randomly
data_shuffled <- train_cv[sample(nrow(train_cv)),]

set.seed(1) 
#Create 10 equally size folds
folds <- cut(seq(1,nrow(data_shuffled)),breaks=10,labels=FALSE)


check_accuracy = function(X){
  
  ## Set value of kfolds
  k <- 10
  
  ## Empty list to store accuracy outputs from various C values  
  accuracy_list <- vector(mode = "list") 
  
  #Perform 10 fold cross validation
  for(i in 1:k){
    #Segement your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    validationData <- data_shuffled[testIndexes, ]
    trainData <- data_shuffled[-testIndexes, ]
    
    kknn.model <- kknn(V11~., 
                       trainData[,1:11], 
                       validationData[,1:11], 
                       k = X,
                       scale = TRUE)
    preds <- fitted(kknn.model)
    accuracy <- sum(preds == validationData[,11]) / nrow(validationData)
    accuracy_list <- c(accuracy_list, accuracy)
    
  }
  
  final_C_outputs <- mean(as.numeric(accuracy_list))
  return(final_C_outputs)
}


acc <- rep(0,20) # set up a vector of 20 zeros to start
for (X in 1:100){
  acc[X] = check_accuracy(X) # test knn with X neighbors
}
acc

## Store the output of accuracy as a data.frame
as.data.frame(acc)

X<-seq(1,100)

## Combine the sequence of x and the accuracy values for ggplot next
combined <- do.call(rbind, Map(data.frame, A=X, B=acc))

## take a look at combined
combined

## Plot x against the accuracy values (k=1 gives us the highest percentage accuracy)
ggplot(data=combined,aes(A,B))+geom_point()




set.seed(1)
## using k=1 train the model on training+validation data and predict on unseen test data 
kknn.model <- kknn(V11~., 
                   train_cv[,1:11], ## this is the full training dataset
                   test_cv[,1:11], ## data the model has never seen before
                   k = 1,
                   scale = TRUE)
set.seed(1)
## Predict on the test_cv data that the model has never seen before
preds <- fitted(kknn.model)

## Final accuracy of 81.5% (as expected, smaller than what we saw in the testing scenarios above)
sum(preds == test_cv[,11]) / nrow(test_cv)


################################################################################

## Problem 3.1 (b) USING SVM

install.packages("kernlab")
library(kernlab)
library(ggplot2)
data <- read.table("credit_card_data.txt")

## Look at a sample of the data
head(data)

set.seed(1)
## Take a random sample of 60% of the data aside for training
mask_train <- sample(nrow(data), size = floor(nrow(data) * 0.6))

## Store this data in the 'train' dataframe
train <- data[mask_train,]

## Store the remaining 40% as 'leftover'
leftover <- data[-mask_train,]

## Divide the leftover by 2 and place one in validation and one in test
validation <- leftover[1:(nrow(leftover)/2),]
test <- leftover[((nrow(leftover)/2)+1):nrow(leftover),]

## make the results reproducible
set.seed(1)

## Create a sequence of values in magnitudes of 10 from 1e-08 to 1e+08 (17 values to test)
x <- 10^seq(-8, 8, 1)


## Look at how different values of 'x' passed to the 'C' argument in the ksvm model produce 
## different accuracy percentages on the validation data
accuracy <-sapply(x, function(x){
set.seed(1)
model_scaled <- ksvm(V11~.,data=train, ## Use the 60% training data
                     type = "C-svc", # Use C-classification method
                     kernel = "rbfdot",
                     C = x,
                     scaled=TRUE) # have ksvm scale the data for you

set.seed(1)
##  Predict on the validation data (20% of our 654 rows)
pred_scaled <- predict(model_scaled,validation[,1:10])

## Calculate the accuracy
sum(pred_scaled == validation$V11) / nrow(validation)
})

## Store the output of accuracy as a data.frame
as.data.frame(accuracy)

## Combine the sequence of x and the accuracy values for ggplot next
combined <- do.call(rbind, Map(data.frame, A=x, B=accuracy))

## take a look at combined
combined

## Plot x against the accuracy values
ggplot(data=combined,aes(A,B))+geom_point()



## It appears that a C value of 0.1 and 1 produce the highest accuracy of 82.44% when 
## looking at the validation data

set.seed(1)
## Now train the model using the determined value of C = (0.1)
model_highest_C <- ksvm(V11~.,data=train,
                     type = "C-svc", # Use C-classification method
                     kernel = "rbfdot", 
                     C = 0.1,
                     scaled=TRUE) # have ksvm scale the data for you

## Predict on an entirely unfamiliar dataset--'test'  the other 20% of our 
## 654 rows of data
set.seed(1)
pred_highest_C <- predict(model_highest_C,test[,1:10])

## What was the final accuracy of our model? In this case 90.8% 
sum(pred_highest_C == test$V11) / nrow(test)


####################################################################################
Problem 4.1
In marketing analytics, part of my job as an analyst is to improve user engagement with advertisements.  Using a clustering analysis to group users in to audience segments is a very hot “buzzword.”  Business leaders are always trying to break our audience in to meaningful segments.  Using a clustering analysis could do this!  Several predictors that might help could be: time spent,  device type, day of week, time of day, refer type (where did the user come from prior to hitting out site?).  
